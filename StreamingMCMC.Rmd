---
title: "Streaming MCMC"
---

Markov Chain Monte Carlo (MCMC) is a method used to sample from an intractable posterior probability distribution. In general it is used to approximate high-dimensional integrals that can't be calculated analytically.

In order to estimate the parameters of a Partially Observed Markov Process (POMP) model, we use Particle MCMC methods (see [Doucet et al 2010](http://www.stats.ox.ac.uk/~doucet/andrieu_doucet_holenstein_PMCMC.pdf)). This involves estimating the marginal likelihood of the process using a Particle Filter, then using the pseudo-marginal likelihood in a [Metropolis hastings](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) algorithm. 

# The PMMH Algorithm

The Particle Marginal Metropolis-Hastings algorithm (PMMH)~\citep{Andrieu2010} is an offline Markov chain Monte Carlo (MCMC) algorithm which targets the full joint posterior $p(\textbf{x}(t_{0:M}), \theta | y(t_{1:M}))$ of a partially observed Markov process. Consider a POMP model given by,

$$
\begin{align*}
Y(t_i)|\eta(t_i) &\sim \pi(y(t_i) | \eta(t_i), \theta), \\
\eta(t_i)|\textbf{x}(t_i) &= g(F^T_{t_i} \textbf{x}(t_i)), \\
\textbf{X}(t_i)|\textbf{x}(t_{i-1}) &\sim p(\textbf{x}(t_i) | \textbf{x}(t_{i-1}), \theta), \quad \textbf{x}(t_0) \sim p(\textbf{x}(t_0)|\theta),
\end{align*}
$$

where $\theta$ represents the parameters to be estimated using the PMMH algorithm. The parameters include the measurement noise in the observation distribution, $\pi(y(t_i) | \eta(t_i), \theta)$, the parameters of the Markov transition kernel for the system state, $p(\textbf{x}(t_i) | \textbf{x}(t_{i-1}), \theta)$ and the parameters of the initial state distribution $p(\textbf{x}(t_0)|\theta)$. The data, $y(t)$, is observed discretely. In order to simulate a Markov chain which targets the full posterior, $p(\theta, \textbf{x} | y)$, firstly a new set of parameters $\theta^*$ is proposed from a proposal distribution $q(\theta^*|\theta)$. Then the bootstrap particle filter (see Section~\ref{sec:bootstrap}), is run over all of the observed data up to time $t$ using the newly proposed $\theta^*$. The output of running the filter with the new set of parameters, $\theta^*$ is used to estimate the marginal likelihood, $\hat{p}_{\theta^*}(y) = \prod_{i=1}^n \hat{p}_{\theta^*}(y(t_i)|y(t_{i-1}))$ and, optionally, to sample a new proposed system state, $x^*$ from the conditional distribution $p(x^* | \theta^*, y)$. The pair $(\theta^*, x^*)$ are accepted with probability $\text{min}(1, A)$, where $A$ is given by:

$$
\begin{equation}
\label{eqn:metropRatio}
A = \frac{p(\theta^*)\hat{p}_{\theta^*}(y) q(\theta^*|\theta)}{p(\theta)\hat{p}_{\theta}(y)q(\theta|\theta^*)},
\end{equation}
$$

the distribution $p(\theta)$, represents the prior distribution over the parameters.

The Metropolis-Hastings Kernel, can be simplified in the case of a symmetric proposal distribution. For a symmetric proposal distribution $q(\theta^*|\theta) = q(\theta|\theta^*)$. Commonly, the proposal is chosen to be a Normal distribution centered at the previously selected parameter, this is know as a random walk proposal, $q(\theta^*|\theta) = \mathcal{N}(\theta, \sigma)$, where $\sigma$ is a parameter controlling the step size of the random walk. If a flat prior distribution is chosen, then the ratio can be simplified further to:

$$
\begin{equation*}
A = \frac{\hat{p}_{\theta^*}(y)}{\hat{p}_{\theta}(y)}.
\end{equation*}
$$

The full-joint posterior distribution is explored by performing many iterations of the PMMH algorithm, discarding burn-in iterations and possibly thinning the iterations to get less correlated samples from the posterior distribution.

## PMMH As A Stream

The Particle Marginal Metropolis-Hastings algorithm must be applied to a batch of data. Window functions, such as `grouped`, can be applied to a stream of data to aggregate observations into a batch. `grouped` accepts an integer, $n$, and groups each observation into another (finite) stream of size $n$.

The PMMH algorithm can then be applied to the aggregated group using `map`. Iterations from the PMMH algorithm are naturally implemented as a stream. In the Scala standard library there is a method for producing infinite streams from an initial seed:

```scala
def iterate[A](start: A)(f: A => A): Stream[A] 
```

`iterate` applies the function `f` to the starting value, then passes on the result to the next evaluation of `f`. For example, to create an infinite stream of natural numbers:

```scala
val naturalNumbers: Stream[Int] = iterate(1)(a => a + 1)
```

Iterations of an MCMC algorithm can be generated using `iterate`, by starting with an initial value of the required state (at a minimum the likelihood and the initial set of parameters) and applying the Metropolis-Hastings update at each iteration. Inside of each application of `f`, a new value of the parameters is proposed, the marginal likelihood is calculated using the new parameters (using the bootstrap particle filter) and the Metropolis-Hastings update is applied.

An illustrative example of a single step in the PMMH algorithm using the Metropolis Kernel is presented in the code block below. Three important functions are given abstract implementations in the `MetropolisHastings` trait, `proposal`, `prior` and `logLikelihood`. The `proposal: Parameters => Rand[Parameters]` is a function representing the (symmetric) proposal distribution, `Rand` is a representation of a distribution which can be sampled from by calling the method `draw`. `logLikelihood: Parameters => LogLikelihood` is a particle filter, with the observed data and number of particles fixed, which outputs an estimate of the log-likelihood for a given value of the parameters. `prior: Parameters => LogLikelihood` represents the prior distribution over the parameters. These three functions will be implemented in a concrete class extending the `MetropolisHastings` trait and correspond to specific implementation of the PMMH algorithm.

```scala
trait MetropolisHastings {
    import MetropolisHastings._
    import math._
    import breeze.stats.distributions.{Rand, Uniform}

    val prior: Parameters => LogLikelihood
    val proposal: Parameters => Rand[Parameters]
    val logLikelihood: Parameters => LogLikelihood

    val stepMetrop: MetropState => MetropState = s => {
        val propParams = proposal(s.params).draw
        val propll = logLikelihood(propParams)
        val a = propll + prior(propParams) - s.ll - prior(s.params)

        if (log(Uniform(0, 1).draw) < a)
            MetropState(propll, propParams)
        else s
    }
}

object MetropolisHastings {
    type Parameters = Vector[Double]
    type LogLikelihood = Double

    case class MetropState(ll: LogLikelihood, params: Parameters) 
}
```

In order to generate a stream of iterations, use `iterate`:

```scala
val initState = MetropState(-1e99, initParams)
val iters = iterate(initState)(stepMetrop)
```

Where `initParams` are drawn from the prior distribution and the initial value of the log-likelihood is chosen to be very small so the first iteration of the PMMH is accepted.

Built in stream operations can be used to discard burn-in iterations and thin the iterations to reduce auto-correlation between samples. The stream can be written to a file or database at each iteration, so the PMMH algorithm implemented as a stream uses constant memory as the chain size increases.

## Working with a Stream of MCMC Iterations

By default, the iterations from the PMMH algorithm are an Akka stream. Since the parameters are a stream, we can perform streaming operations on parameters, such as thinning:

```scala
def thinParameters[A](thin: Int) = {
    Flow[A].zip(Source(Stream.from(1))).
      filter{ case (_, iteration) => iteration % thin == 0}.
      map{ case (p, _) => p }
  }
```

The stream of parameters is zipped to a stream of integers, `1 2 3 ...`, and then filtered, finally the integers are removed by a call to map. MCMC algorithms often take a little while to properly start exploring the posterior, these iterations can be discarded by calling `drop`.

```scala
def burnin[A](burn: Int) = {
    Flow[A].drop(burn)
  }
```

Sometimes a complex model with many free parameters can require a large amount of iterations from the PMMH algorithm to determine the parameter posterior. In this case, we would like a way to stream the iterations to a file in order to avoid running out of memory. With Akka streams, we can asynchronously write to a file using the 
`fileIO` sink:

```scala
params.
  map{ p => ByteString(s"$p\n") }.
  runWith(FileIO.toFile(new File("output.csv")))
```

A `toString` method in the `Parameters` class gives the CSV output. In order to determine the state space online using the parameter posterior, a suitable summary of the parameters must be provided for use the with [[particle filter|The-Particle-Filter]]. The mean of the parameters is a suitable value.

First, consider how to calculate the average of parameters from a file. Since the iterations of the PMMH algorithm are written to a file, we must read in and parse the parameters. Consider a simple linear model:

```scala
  val p = LeafParameter(GaussianParameter(3.0, 2.0), Some(1.0), BrownianParameter(0.1, 1.0))
  val mod = LinearModel(stepBrownian)

  val times = (0.0 to 50.0 by 0.5).toList
  val sims = simData(times, mod(p))
```

Now we have simulated data from a linear model, with a Gaussian observation model, let's recover the parameters using Particle Marginal Metropolis Hastings:

```scala
val mll = pfMll(sims, mod)(500)
ParticleMetropolis(mll, p, Parameters.perturb(1.0)).iters.
  take(10000).
  map( p => ByteString(s"$p\n")).
  runWith(FileIO.toFile(new File("LinearMCMC.csv")))
```

Now we can read back in the file `LinearMCMC.csv`, parse, drop burnin terms and thin the stream before calculating the mean:

```scala
  val future = cleanParameterFlow(new File("LinearMCMC.csv"), burnin = 1000, thin = 2, totalIterations = 10000)
  val p: Vector[Double] = Await.result(p.run, 1 second)
  val params = Parameter(GaussianParameter(p.head, p(1)), Some(p(2)), BrownianParameter(p(3), p(4)))
```

`cleanParameterFlow` returns a future, since it could be reading an unbounded file. So we force it to return a result and parse the vector to the set of parameters required for the Linear Model.

## PMMH Diagnostics

In practice it is desirable to run multiple chains from different starting values (possibly with different prior distributions), with different proposal distributions or with different number of particles in order to establish whether the chain will converge on the correct posterior distribution. Since individual chains are independent, we can run them in parallel, which is easy using [Akka Streams](https://akka.io).

```scala
val params = // initial parameters
val mll = // a particle filter, function from Parameters => LogLikelihood
val particles = Vector(100, 200, 500, 1000)
val iterations = 10000

Source(particles).
  mapAsync(parallism = 4).
  map { p => 

    // initialise a PMMH algorithm with different amounts of particles
    val mh = ParticleMetropolis(mll(particles), params, Parameters.perturb(0.1))
    runMcmc(mh, s"exampleMCMC-$particles", iterations).run()
    
  }.
  runWith(Sink.onComplete { _ =>
    system.terminate()
  })
```

A full suite of diagnostic tools can be found in the R package [coda](https://cran.r-project.org/web/packages/coda/index.html). Since the MCMC iterations are written to a csv, this can easily be imported using R and analysed using coda.  