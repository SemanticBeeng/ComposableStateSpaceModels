---
title: "Composable State Space Models"
author: "Jonathan Law"
date: "22 August 2016"
output: html_document
---

```{r, echo=FALSE, warning=FALSE, message=FALSE, background=TRUE, results='hide'}
packages <- c("dplyr", "tidyr", "ggplot2", "gridExtra", "magrittr", "tidyr", "coda", "ggmcmc", "readr")
newPackages <- packages[!(packages %in% as.character(installed.packages()[,"Package"]))]
if(length(newPackages)) install.packages(newPackages)
lapply(packages,require,character.only=T)

theme_set(theme_minimal())
```

# ComposableStateSpaceModels

This is a Scala library for continuous time partially observed Markov processes (POMP). Partially observed Markov processes can be used to model time series data, allowing interpolation and forecasting. 

Introduction to Partially Observed Markov Process Models
--------------------------------------------------------

Partially observed Markov processes are a type of [State Space Model](https://en.wikipedia.org/wiki/State-space_representation). This means the models feature unobserved, or latent, variables. The unobserved system state is governed by a [diffusion process](https://en.wikipedia.org/wiki/Diffusion_process), these are continuous time Markov processes meaning that future values of the state space, are independent from all previous values given the current state, x(t). A representation of a POMP model as a directed acyclical graph is below.

<!-- ![POMP DAG](Figures/PompDag.png) -->

The distribution, p, represents the Markov transition kernel of the state space. The distribution pi, represents the observation distribution, parameterised by the state space. The function f is a linear deterministic function, which can be used to add cyclic seasonal components to the state space. The function g is the linking-function from a [generalised linear model](https://en.wikipedia.org/wiki/Generalized_linear_model), which transforms the state space into the parameter space of the observation model. Define gamma = f(t, x) and eta = g(gamma).

## Simulating the State Space

A typical diffusion process is the [Ornstein-Uhlenbeck process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process), which can be simulated by specifying the parameters of the process, `theta`, the mean of the process, `alpha` how quickly the process reverts to the mean and `sigma` the noise of the process. Then we must specify an initial state, which is done by drawing from a Gaussian distribution, since the exact solution to the OU process is a Gaussian distribution. Then we pass a `stepFunction` containing the exact solution to the OU process, relying on only the previous value of the realisation (because the process is  Markovian) and the time difference between realisations.

```scala
import model.StateSpace._
import model.State._
import model.OrnsteinParameter
import breeze.stats.distributions.Gaussian
import model.SimData._

val p = OrnsteinParameter(theta = 6.0, alpha = 0.05, sigma = 1.0)
val initialState = LeafState(Vector(Gaussian(6.0, 1.0).draw))

val sims = simSdeStream(initialState, 0.0, 300.0, 1, stepOrnstein(p))
```

Notice, the state space can be multidimensional, and as such is represented by a `Vector`. A single state is represented by a LeafState, this will become clear when considering composition of models. The figure below shows a representation of the Ornstein-Uhlenbeck process with `theta = 6.0, alpha = 0.05, sigma = 1.0`.

```{r ornstein, echo=FALSE}
system("cd ../ && sbt \"run-main examples.SimulateOrnstein\"")
orn = read_csv("../OrnsteinSims.csv", col_names = c("Time", "Value"))

orn %>%
  ggplot(aes(x = Time, y = Value)) + geom_line() +
  ggtitle("Ornstein-Uhleneck Process")
```

## Simulating a Single Model

The observations of a POMP can be from any parameterised distribution. The observation distribution depends on the latent variables and sometimes on additional parameters not in the system state, such as a scaling parameter representing measurement noise. A simple non-gaussian observation model, used for representing count data is the Poisson distribution, parameterised by it's rate $\lambda(t)$. If we consider the rate, $\lambda(t)$ to vary stochastically, then we can represent it using a POMP mode. Firstly select a representation of the state space, we will use the Ornstein-Uhlenbeck process from the previous example, then specify the parameters and the times the process should be observed. There is a `LeafParameter` class which combines the initial State, optional scale parameter and the state space parameters for a single model.


```scala
import model.{LeafParameter, GaussianParameter, OrnsteinParameter}
import model.SimData._
import model.POMP.PoissonModel
import model.StateSpace._

val p = LeafParameter(
  GaussianParameter(-2.0, 1.0),
  None,
  OrnsteinParameter(theta = 2.0, alpha = 1.0, sigma = 1.0))
val mod = PoissonModel(stepOrnstein)


val times = (1.0 to 100.0 by 1.0).toList
val sims = simData(times, mod(p))
```

```{r, echo=FALSE}
system("cd ../ && sbt \"run-main examples.SimulatePoisson\"")
poisson = read_csv("../PoissonSims.csv", 
                col_names = c("Time", "Value", "Eta", "Gamma", "State"))

p1 = poisson %>%
  ggplot(aes(x = Time, y = Value)) + geom_step() + 
  ggtitle("Poisson Observations")

p2 = poisson %>%
  dplyr::select(Time, Eta, State) %>%
  gather(key = "key", value = "value", -Time) %>%
  ggplot(aes(x = Time, y = value, linetype = key)) + geom_line() + 
  facet_wrap(~key, ncol = 1, scales = "free_y") + 
  theme(legend.position="none") + facet_wrap(~key, ncol = 1, scales = "free")

grid.arrange(p1, p2, heights = c(1,2))
```

The figure shows the state space, which varies along the whole real line and the transformed state space and Eta, which varies in $\mathbb{R}^+$ The linking function, g, is the log-link.

## Composing Multiple Models

If we wish to consider more complex process, for instance a Poisson model with a seasonally varying rate, then we have to add deterministic values to the state before applying the observation distribution. The function, f, is a linear deterministic function which can be used to add seasonal factors to the system state. 

Models are represented as a function from `Parameters => Model`, this means models are defined unparameterised. A function for combining two unparameterised models is `Model.combine`, this function is associative, but not commutative. This is because the function selects the leftmost Model's observation and linking functions. The code snippet below shows how to construct a seasonal Poisson model, the observation distribution is Poisson, but the rate of an event occuring follows a daily (period T = 24) cycle if we assume count observations are made once every hour.

```scala
  val poissonParams = LeafParameter(
    GaussianParameter(0.0, 1.0),
    None,
    BrownianParameter(0.1, 0.3))
  val seasonalParams = LeafParameter(
    GaussianParameter(DenseVector(Array.fill(6)(0.0)),
      diag(DenseVector(Array.fill(6)(1.0)))),
    None,
    BrownianParameter(Vector.fill(6)(0.1), Vector.fill(6)(0.4)))

  val params = poissonParams |+| seasonalParams
  val mod = BernoulliModel(stepBrownian) |+| SeasonalModel(24, 3, stepBrownian)

  val times = (1.0 to 100.0 by 1.0).toList
  val sims = simData(times, mod(params))
```

```{r, echo=FALSE}
system("cd ../ && sbt \"run-main examples.SimulateSeasonalPoisson\"")
seasPois = read.csv("../seasonalPoissonSims.csv", header = F,
                    col.names = c("Time", "Value", "Eta", "Gamma", sapply(1:7, function(i) paste("State", i, sep = ""))))

p1 = seasPois %>%
  ggplot(aes(x = Time, y = Value)) + geom_step() + 
  ggtitle("Poisson Observations")

p2 = seasPois %>%
  dplyr::select(Time, Eta, Gamma) %>%
  gather(key = "key", value = "value", -Time) %>%
  ggplot(aes(x = Time, y = value, colour = key)) + geom_line() + 
  facet_wrap(~key, ncol = 1, scales = "free_y") + theme(legend.position="none")

p3 = seasPois %>%
  dplyr::select(-Value, -Eta, -Gamma) %>%
  gather(key = "key", value = "value", -Time) %>%
  ggplot(aes(x = Time, y = value, colour = key)) + geom_line() + theme(legend.position = "none")

grid.arrange(p1, p2, p3, heights = c(1,2,1))
```


## Statistical Inference: The Particle Filter

If we have a fully specified model, ie the posterior distributions of the parameters given the data so far are available to us, then we can use a bootstrap particle filter (see [Sequential Monte Carlo Methods in Practice](https://www.springer.com/us/book/9780387951461)) to determine the hidden state space of the observations. Consider the simulated Bernoulli model, the parameters are given by:

```scala
val p = LeafParameter(
  GaussianParameter(6.0, 1.0),
  None,
  OrnsteinParameter(theta = 6.0, alpha = 0.05, sigma = 1.0))
```

We can consider the point values of the parameters to be the mean of the posterior parameter distribution. Then the bootstrap particle filter can be applied to the simulated data and the inferred state space can be compared to the previously simulated state space. The data can be read in from a CSV or database, or simulated again. However, since these are stochastic models we can't compare different realisations of the same model. The particle filter is using 1,000 particles.

```scala
  val data = // poisson data

  val p = LeafParameter(
    GaussianParameter(6.0, 1.0),
    None,
    OrnsteinParameter(theta = 6.0, alpha = 0.05, sigma = 1.0))
  
  val mod = BernoulliModel(stepOrnstein)

  // Define the particle filter
  val filter = Filter(mod.model, ParticleFilter.multinomialResampling, data.map(_.t).min)

  // Run the particle filter over the observed data using 1,000 particles
  val filtered = filter.filterWithIntervals(data)(1000)(mod.params)
```

The figure below shows the actual simulated state, plotted next to the estimate state and 99% [credible intervals](https://en.wikipedia.org/wiki/Credible_interval).


```scala
  val data = // poisson data

  val p = LeafParameter(
    GaussianParameter(6.0, 1.0),
    None,
    OrnsteinParameter(theta = 6.0, alpha = 0.05, sigma = 1.0))

  // declare a new filter type including the model, resampling scheme and starting time, t0
  val filter = Filter(mod.model, ParticleFilter.multinomialResampling, t0 = 0.0)
  
  // use the filter to return the state and credible intervals with 1000 particles
  filter.filterWithIntervals(data)(1000)(mod.p)
```

```{r, echo = FALSE}
system("cd ../ && sbt \"run-main examples.FilterPoisson\"")
poisson = read_csv("../PoissonSims.csv", col_names = c("Time", "Observation", "Eta", "Gamma", "State"))
poissonFiltered = read_csv("../PoissonFiltered.csv",
                        col_names = c("Time", "Value", "PredEta", "lowerEta", "upperEta", "PredState", "Lower", "Upper"))

p1 = poisson %>%
  dplyr::select(-Gamma, -Eta, -Observation) %>%
  inner_join(poissonFiltered[,-2], by = "Time") %>%
  select(Time, State, PredState, Lower, Upper) %>%
  gather(key = "key", value = "value", -Time, -Lower, -Upper) %>%
  ggplot() + geom_line(aes(x = Time, y = value, linetype = key)) +
  geom_ribbon(aes(x = Time, ymin = Lower, ymax = Upper), alpha = 0.3)

p2 = poisson %>%
  dplyr::select(-Gamma, -State, -Observation) %>%
  inner_join(poissonFiltered[,-2], by = "Time") %>%
  select(Time, Eta, PredEta, lowerEta, upperEta) %>%
  gather(key = "key", value = "value", -Time, -lowerEta, -upperEta) %>%
  ggplot() + geom_line(aes(x = Time, y = value, linetype = key)) +
  geom_ribbon(aes(x = Time, ymin = lowerEta, ymax = upperEta), alpha = 0.3)

grid.arrange(p1, p2, ncol = 1)
```

## Inference for the Full Joint Posterior Distribution

Say we have observed a time depending process in the real world, and don't have the parameters available for the model. We wish to carry out inference for the state space and the parameters of the model simultaneously. This framework implements the Particle Marginal Metropolis Hastings (PMMH) Algorithm (see [Doucet et al. 2010](http://www.stats.ox.ac.uk/~doucet/andrieu_doucet_holenstein_PMCMC.pdf)). The likelihood of the state space and parameters given the observations can be determined using a particle filter, then a standard Metropolis-Hastings update step is used to create a Markov Chain representing the full join posterior of the model given the observed real-world process.

Now we can implement the PMMH algorithm for the simulated Bernoulli observations, and determine if the algorithm is able to recover the parameters.

```scala
val data = // poisson data

val p = LeafParameter(
  GaussianParameter(6.0, 1.0),
  None,
  OrnsteinParameter(theta = 6.0, alpha = 0.05, sigma = 1.0))

val mod = PoissonModel(stepOrnstein)

// build the particle filter by selecting the model type and resampling scheme
val filter = Filter(mod, ParticleFilter.multinomialResampling, 0.0)

// specify the filter type (llFilter, to return estimate of log-likelihood),
// the number of particles and observations
val mll = filter.llFilter(data)(particles = 1000) _

// build the PMMH algorithm using mll estimate (via particle filter), the
// initial parameters and the proposal distribution for new paramters
val mh = ParticleMetropolis(mll, p, Parameters.perturb(delta))

// run the PMMH as an akka stream in parallel (2 chains) and write the results to a file
runPmmhToFile(s"PoissonSimParams-$delta-$particles", 2, mod.p, mll, Parameters.perturb(0.05), 10000)
```

```{r, echo=FALSE, eval = FALSE}
system("cd ../ && sbt \"run-main examples.GetPoissonParameters 10000 1000 0.05\"")
iters = read_csv("PoissonSimParams-0.05-1000.csv", col_names = c("mu", "sigma", "theta", "alpha", "sigma"))

plotIters = function(iters, variable, thin = 10, burning = nrow(iters)*0.1) {
  mcmcObject = mcmc(iters[seq(from = burnin, to = nrow(iters), by = thin), variable]) %>% ggs()
  
  p1 = ggs_histogram(mcmcObject)
  p2 = ggs_traceplot(mcmcObject)
  p3 = ggs_autocorrelation(mcmcObject)
  p4 = ggs_running(mcmcObject)
  
  grid.arrange(p1, p2, p3, p4)
}


mcmc(iters) %>% plot()
```

Note that the algorithm has been initialised at the same parameter values we used to simulate the model, this kind of prior information is not typically known for real world processes, unless similar processes have been extensively studied. 

## Online Monitoring of MCMC

For large, complex models requiring many parameters the MCMC run may take a long time. It is of interest to monitor the MCMC chain to see if it is converging. The function keeps track of the acceptance ratio and the variance of the estimate of the marginal log-likelihood, which can be used to [tune the PMMH algorithm](https://darrenjw.wordpress.com/2014/06/08/tuning-particle-mcmc-algorithms/). As a rule of thumb, 30% acceptance ratio and marginal log-likelihood variance of 1 is ideal for convergence of the MCMC algorithm. In order to monitor the PMMH run for the bernoulli data, we need to modify it as such:

```scala
val iters = ParticleMetropolis(mll, gaussianPerturb(0.5, 0.1)).iters(p)

iters.
  via(monitorStream(1000, 1)).
  runWith(Sink.ignore)

iters.
  map(s => s.params).
  take(iterations).
  map( p => ByteString(s"$p\n")).
  runWith(FileIO.toFile(new File("BernoulliMCMC.csv")))
```

## Running Multiple Chains

We have provided a convenience function to run multiple chains (in parallel) from the same initial starting position, with the same number of iterations and particles. However it can be easily adapted to run multiple chains with different starting parameters, or multiple chains with different number of particles and / or proposal distributions. The main work is done using the [Akka Streams](http://akka.io/) library.


```scala
// initialise the actor system and materializer
import scala.concurrent.ExecutionContext.Implicits.global
implicit val system = ActorSystem("MultipleChains")
implicit val materializer = ActorMaterializer()

// specify the number of iterations, particles and filename prefix
val iterations = 10000
val particles = 200
val fileOut = "TestMCMC"

// Source defines a new stream, this stream is a range from 1 to 4
// if we want to define multiple starting points, we can provide a vector of Parameters to Source
// then map over them in the mapAsync call
Source(1 to 4).
  mapAsync(parallelism = 4){ chain =>
    val iters = ParticleMetropolis(mll(particles), initParams, Parameters.perturb(0.1)).iters

    iters.
      zip(Source(Stream.from(1))).
      map{ case (x, i) => (i, x.params) }.
      take(iterations).
      map{ case (i, p) => ByteString(s"$i, $p\n") }.
      runWith(FileIO.toFile(new File(s"$fileOut-$iterations-$particles-$chain.csv")))
    
    iters.
      via(monitorStream(1000, chain)).
      runWith(Sink.ignore)
  }.
  runWith(Sink.onComplete { _ =>
    system.shutdown()
  })
```

This function above will run 4 chains, on four threads, and write them to individual files whilst printing convergence diagnostics every 1000th iteration. The convergence diagnostics can also be written to a file, using `runWith(FileIO.toFile)` as is done with the parameter iterations.

For more usage examples see the [examples](src/main/scala/examples) directory.
